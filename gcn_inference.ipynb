{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bab47f22-5cb0-465b-ba49-ba4d143b3185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded test embeddings from bert_embeddings/X_test.npy\n",
      "Loaded label binarizer from bert_embeddings/mlb.pkl\n",
      "Loaded GCN model from GCN_model_optmizers/GCN_model.pth\n",
      "Loaded optimal thresholds from GCN_model_optmizers/optimizer.pth\n",
      "Loaded test data from test_revised.json\n",
      "Loaded relation mapping from 'rel_info.json'\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Imports -------------------\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "from itertools import product\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, knn_graph\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "# ------------------- Load All Necessary Files -------------------\n",
    "# Set device to CPU only\n",
    "DEVICE = \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Load test embeddings\n",
    "test_embeddings = np.load(\"bert_embeddings/X_test.npy\")\n",
    "print(f\"Loaded test embeddings from bert_embeddings/X_test.npy\")\n",
    "\n",
    "# Load MultiLabelBinarizer\n",
    "with open(\"bert_embeddings/mlb.pkl\", 'rb') as file:\n",
    "    label_binarizer = pickle.load(file)\n",
    "print(f\"Loaded label binarizer from bert_embeddings/mlb.pkl\")\n",
    "\n",
    "# Define GCN model class\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim, dropout=0.4):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim1)\n",
    "        self.conv2 = GCNConv(hidden_dim1, hidden_dim2)\n",
    "        self.conv3 = GCNConv(hidden_dim2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Load trained GCN model with map_location to force CPU\n",
    "input_dim = test_embeddings.shape[1]  # 768 (BERT embedding size)\n",
    "hidden_dim1 = 512\n",
    "hidden_dim2 = 256\n",
    "output_dim = len(label_binarizer.classes_)\n",
    "gnn_model = GCN(input_dim, hidden_dim1, hidden_dim2, output_dim, dropout=0.4).to(DEVICE)\n",
    "gnn_model.load_state_dict(torch.load(\"GCN_model_optmizers/GCN_model.pth\", map_location='cpu'))\n",
    "gnn_model.eval()\n",
    "print(f\"Loaded GCN model from GCN_model_optmizers/GCN_model.pth\")\n",
    "\n",
    "# Load optimal thresholds with map_location to force CPU\n",
    "optimal_thresholds = torch.load(\"GCN_model_optmizers/optimizer.pth\", map_location='cpu')\n",
    "optimal_thresholds = np.zeros(output_dim) if not isinstance(optimal_thresholds, np.ndarray) else optimal_thresholds\n",
    "print(f\"Loaded optimal thresholds from GCN_model_optmizers/optimizer.pth\")\n",
    "\n",
    "# Load test data\n",
    "with open(\"test_revised.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    test_data = json.load(file)\n",
    "print(f\"Loaded test data from test_revised.json\")\n",
    "\n",
    "# ------------------- Load Relation Mapping -------------------\n",
    "def load_relation_mapping(file_path=\"rel_info.json\"):\n",
    "    \"\"\"Load relation mapping from JSON file.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "relation_mapping = load_relation_mapping()\n",
    "print(\"Loaded relation mapping from 'rel_info.json'\")\n",
    "\n",
    "# ------------------- Real-Time Inference with Top 2 Confidence Filtering -------------------\n",
    "def infer_sample(model, sample_data, sample_embedding, optimal_thresholds, label_binarizer, relation_map, user_sentence):\n",
    "    \"\"\"Perform inference with GCN, limiting each entity pair to its top 2 highest-confidence relations.\"\"\"\n",
    "    sentence = user_sentence.strip()\n",
    "\n",
    "    # Extract true triplets and relations from the full sample\n",
    "    true_triplets = [(rel[\"h\"], rel[\"r\"], rel[\"t\"]) for rel in sample_data.get(\"labels\", [])]\n",
    "    true_relations = set(rel[\"r\"] for rel in sample_data.get(\"labels\", []))\n",
    "\n",
    "    # Map entity indices to names from vertexSet\n",
    "    vertex_set = sample_data.get(\"vertexSet\", [])\n",
    "    if vertex_set and isinstance(vertex_set[0], list) and vertex_set[0] and isinstance(vertex_set[0][0], dict):\n",
    "        entities = {i: sublist[0][\"name\"] for i, sublist in enumerate(vertex_set) if sublist}\n",
    "    else:\n",
    "        entities = {}\n",
    "        print(\"Warning: vertexSet missing or malformed. Using indices instead.\")\n",
    "\n",
    "    # Map true triplets to entity names\n",
    "    true_triplets_mapped = [(entities.get(head, str(head)), relation_map.get(rel, rel), entities.get(tail, str(tail)))\n",
    "                            for head, rel, tail in true_triplets]\n",
    "\n",
    "    # Check if an entity appears in the sentence with word boundaries\n",
    "    def entity_in_sentence(entity, sentence):\n",
    "        return re.search(r'\\b' + re.escape(entity) + r'\\b', sentence, re.IGNORECASE) is not None\n",
    "\n",
    "    # Filter true triplets to those with head and tail in the user sentence\n",
    "    true_triplets_filtered = [triplet for triplet in true_triplets_mapped\n",
    "                              if entity_in_sentence(triplet[0], sentence) and entity_in_sentence(triplet[2], sentence)]\n",
    "\n",
    "    # Identify entities present in the user sentence\n",
    "    sentence_entities = [entity for idx, entity in entities.items() if entity_in_sentence(entity, sentence)]\n",
    "\n",
    "    # Prepare graph data for GCN\n",
    "    x = torch.tensor(sample_embedding, dtype=torch.float).reshape(1, -1)  # Shape: [1, 768]\n",
    "    edge_index = knn_graph(x, k=3, loop=False)  # k=3 as per notebook\n",
    "    edge_index = to_undirected(edge_index)\n",
    "    sample_graph = Data(x=x, edge_index=edge_index).to(DEVICE)\n",
    "\n",
    "    # Predict relation probabilities with GCN\n",
    "    with torch.no_grad():\n",
    "        logits = model(sample_graph)\n",
    "        probability_scores = torch.sigmoid(logits).cpu().numpy().flatten()  # Shape: [n_classes]\n",
    "\n",
    "    # Filter to top 5 highest-confidence relations with their scores\n",
    "    relation_probabilities = [(label_binarizer.classes_[idx], prob) for idx, prob in enumerate(probability_scores)\n",
    "                              if prob >= optimal_thresholds[idx]]\n",
    "    top_relation_probabilities = sorted(relation_probabilities, key=lambda x: x[1], reverse=True)[:5]\n",
    "    top_relations_with_scores = {rel: prob for rel, prob in top_relation_probabilities}\n",
    "\n",
    "    # Generate triplets, limiting each entity pair to its top 2 highest-confidence relations\n",
    "    predicted_triplets = []\n",
    "    for head, tail in product(sentence_entities, repeat=2):\n",
    "        if head != tail:  # Exclude self-relations\n",
    "            # Sort relations by confidence and take top 2\n",
    "            sorted_relations = sorted(top_relations_with_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "            for relation, _ in sorted_relations:\n",
    "                mapped_relation = relation_map.get(relation, relation)\n",
    "                predicted_triplets.append((head, mapped_relation, tail))\n",
    "\n",
    "    # Filter predicted triplets to only those matching true triplets\n",
    "    true_triplets_set = set(true_triplets_filtered)\n",
    "    matched_predicted_triplets = [triplet for triplet in predicted_triplets if triplet in true_triplets_set]\n",
    "\n",
    "    # Calculate match count\n",
    "    correct_triplets = len(matched_predicted_triplets)\n",
    "    total_true_triplets = len(true_triplets_set)\n",
    "    total_predicted_triplets = len(set(predicted_triplets))\n",
    "\n",
    "    return {\n",
    "        \"sentence\": sentence,\n",
    "        \"true_triplets\": list(true_triplets_filtered),\n",
    "        \"predicted_triplets\": matched_predicted_triplets,\n",
    "        \"match_count\": (correct_triplets, total_true_triplets, total_predicted_triplets)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4648f2f6-2564-48fe-bb7a-586b1a9cde41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Test Samples (First 3 Sentences)\n",
      "Sample 1: \"\" Ramblin ' on My Mind \" is a blues song recorded on November 23 , 1936 in San Antonio , Texas by blues musician Robert Johnson . The song was originally released on 78 rpm format as Vocalion 03519 and ARC 7 - 05 - 81 . Johnson performed the song in the key of E , and recorded two takes . ...\"\n",
      "\n",
      "Sample 2: \"Europafilm was an influential Swedish film company established in 1929 by Schamyl Bauman and Gustaf Scheutz . The office was located at Kungsgatan in central Stockholm , while the film studio was located in Mariehäll , Bromma , northwest of Stockholm city . It was acquired by Bonnier in 1984 and merged with Svensk Filmindustri in 1985 . ...\"\n",
      "\n",
      "Sample 3: \"The Wagner – Rogers Bill was proposed United States legislation which would have increased the quota of immigrants by bringing a total of 20,000 Jewish children ( there was no sectarian criteria ) under the age of 14 ( 10,000 in 1939 , and another 10,000 in 1940 ) to the United States from Nazi Germany . The bill was sponsored by Senator Robert F. Wagner ( D - N.Y. ...\"\n",
      "\n",
      "Sample 4: \"ProSieben ( , sieben is German for seven ) is a German free - to - air television network . It was launched on 1 January 1989 . It is Germany 's second - largest privately owned television company . ...\"\n",
      "\n",
      "Sample 5: \"Charles Louis Domanico ( January 20 , 1944 – October 17 , 2002 ) , better known as Chuck Domanico , was an American jazz bassist who played double bass and bass guitar on the West Coast jazz scene . Domanico was born in Chicago . He settled in Los Angeles in the mid-1960s . ...\"\n",
      "\n",
      "Sample 6: \"House of Angels ( ) is a Swedish drama film which was released to cinemas in Sweden on 21 February 1992 , about a little village in Västergötland , Sweden , where an aging recluse lives in a mansion on a large wooded property . One day he is accidentally killed and an unknown relative by the name of Fanny Zander inherits the mansion and land . When she and her friend Zac arrive , they turn life in the staid village upside down . ...\"\n",
      "\n",
      "Sample 7: \"The Guianas , sometimes called by the Spanish loan - word Guayanas ( Las Guayanas ) , are a region in north - eastern South America which includes the following three territories : French Guiana , an overseas department of France Guyana , formerly known as British Guiana from 1831 until 1966 , after the colonies of Berbice , Essequibo , and Demerara , taken from the Netherlands in 1814 , were merged into a single colony Suriname , formerly Dutch Guiana , until 1814 together with Berbice , Essequibo and Demerara . In the wider context the Guianas also additionally include : Guayana Region in Venezuela ( Amazonas , Bolívar , and Delta Amacuro states ) formerly the Guayana Province , alternatively known as Spanish Guyana Portuguese Guiana ( or Brazilian Guiana ) , corresponding to the state of Amapá in northern Brazil .\"\n",
      "\n",
      "Sample 8: \"Altsys Corporation was a Texas - based software company founded by James R. Von Ehr II . It was an early Apple Macintosh developer and publisher . ...\"\n",
      "\n",
      "Sample 9: \"Dieter Eppler ( 11 February 1927 in Stuttgart – 12 April 2008 in Stuttgart ) was a German television actor and director of radio dramas . He was born on February 11 , 1927 in Stuttgart , Germany . He was an actor , known for Jonas ( 1957 ) , The Country Doctor ( 1987 ) and The Last Winter ( 1960 ) . ...\"\n",
      "\n",
      "Sample 10: \"James Whitman \" Jim \" McLamore ( May 30 , 1926 – August 9 , 1996 ) was with David Edgerton responsible for the expansion of the Burger King fast food franchise . McLamore attended Northfield Mount Hermon School before matriculating at Cornell University . McLamore was an employee and also a businessman before . ...\"\n",
      "\n",
      "Sample 11: \"Allen County is a county in the U.S. state of Ohio . As of the 2010 census , the population was 106,331 . ...\"\n",
      "\n",
      "Sample 12: \"Charles Paul Florimond Quef ( 1 November 1873 , Lille – 2 July 1931 , Paris ) was a French organist and composer . He studied at the conservatory in Lille , and later he attended the Paris Conservatory where he studied with Charles - Marie Widor , Louis Vierne and Alexandre Guilmant . From 1895 to 1898 , he was organist of the Église Sainte - Marie - des - Batignolles and in 1898 , organist of the Saint - Laurent church , Paris . ...\"\n",
      "\n",
      "Sample 13: \"Philip Lodewijk Jacob Frederik Sadée ( 7 February 1837 The Hague – 14 December 1904 The Hague ) is an artist who belongs to the Hague School . Sadée started painting at the age of 20 . He studied in The Hague both at the Academy and in the studio of J E J van den Berg ( 1802 – 1861 ) . ...\"\n",
      "\n",
      "Sample 14: \"Howard the Duck is a 1986 American science fiction comedy film directed by Willard Huyck and starring Lea Thompson , Jeffrey Jones , and Tim Robbins . Produced by Gloria Katz and written by Huyck and Katz , with George Lucas as executive producer , the screenplay was originally intended to be an animated film based on the Marvel comic book of the same name , but the film adaptation became live - action because of a contractual obligation . Although several TV adaptations of Marvel characters had aired during the preceding 21 years , this was the first theatrically released feature film , coming after the serial Captain America . ...\"\n",
      "\n",
      "Sample 15: \"Street Fighter X Mega Man , also known as in Japan , is a crossover platform game created by Singaporean fan developer Seow Zong Hui . Initially developed as a fan game , Street Fighter X Mega Man later received support from Capcom , who assisted in the production of the game . Street Fighter X Mega Man was released as a free download from Capcom Unity on December 17 , 2012 . ...\"\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Interactive Inference\n",
      "--------------------------------------------------\n",
      "Instructions:\n",
      "  The samples listed above are randomly selected from 'test_revised.json'.\n",
      "  Please enter a partial or full sentence from any of the 15 samples displayed.\n",
      "  Example: 'The Sims class destroyers were built'\n",
      "  To exit, type 'exit'.\n",
      "  Output will show true triplets and matching predicted triplets.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter sentence part (or 'exit'):  Street Fighter X Mega Man , also known as in Japan , is a crossover platform game created by Singaporean fan developer Seow Zong Hui . Initially developed as a fan game , Street Fighter X Mega Man later received support from Capcom , who assisted in the production of the game . Street Fighter X Mega Man was released as a free download from Capcom Unity on December 17 , 2012 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Sentence:\n",
      "  \"Street Fighter X Mega Man , also known as in Japan , is a crossover platform game created by Singaporean fan developer Seow Zong Hui . Initially developed as a fan game , Street Fighter X Mega Man later received support from Capcom , who assisted in the production of the game . Street Fighter X Mega Man was released as a free download from Capcom Unity on December 17 , 2012 .\"\n",
      "True Triplets:\n",
      "  (Street Fighter X Mega Man, publisher, Capcom)\n",
      "  (Street Fighter X Mega Man, series, Mega Man)\n",
      "  (Street Fighter X Mega Man, publisher, Capcom Unity)\n",
      "  (Mega Man, developer, Capcom)\n",
      "  (Mega Man, publisher, Capcom)\n",
      "  (Street Fighter, developer, Capcom)\n",
      "  (Street Fighter, publisher, Capcom)\n",
      "  (Capcom Unity, developer, Capcom)\n",
      "  (Street Fighter X Mega Man, developer, Capcom)\n",
      "  (Street Fighter X Mega Man, developer, Seow Zong Hui)\n",
      "  (Capcom, country, Japan)\n",
      "  (Street Fighter X Mega Man, platform, Capcom Unity)\n",
      "  (Seow Zong Hui, notable work, Street Fighter X Mega Man)\n",
      "  (Capcom, product or material produced, Mega Man)\n",
      "  (Seow Zong Hui, country of citizenship, Singaporean)\n",
      "  (Street Fighter, production company, Capcom)\n",
      "  (Mega Man, has part, Street Fighter X Mega Man)\n",
      "  (Capcom, located in the administrative territorial entity, Japan)\n",
      "Predicted Triplets (Matching True Triplets):\n",
      "  (Street Fighter X Mega Man, developer, Seow Zong Hui)\n",
      "  (Street Fighter X Mega Man, developer, Capcom)\n",
      "  (Street Fighter X Mega Man, publisher, Capcom)\n",
      "  (Street Fighter X Mega Man, platform, Capcom Unity)\n",
      "  (Street Fighter X Mega Man, publisher, Capcom Unity)\n",
      "  (Capcom, product or material produced, Mega Man)\n",
      "  (Capcom Unity, developer, Capcom)\n",
      "  (Mega Man, developer, Capcom)\n",
      "  (Mega Man, publisher, Capcom)\n",
      "  (Street Fighter, developer, Capcom)\n",
      "  (Street Fighter, publisher, Capcom)\n",
      "Match Count: 11/18 true triplets correctly predicted \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter sentence part (or 'exit'):  exit\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Display 15 Random Test Samples -------------------\n",
    "print(\"\\nRandom Test Samples (First 3 Sentences)\")\n",
    "random_samples = random.sample(list(enumerate(test_data)), min(15, len(test_data)))\n",
    "sample_indices = [idx for idx, _ in random_samples]\n",
    "sample_sentences = {idx: \" \".join([\" \".join(sent) for sent in test_data[idx].get(\"sents\", [])]).strip()\n",
    "                    for idx, _ in random_samples}\n",
    "\n",
    "for i, (idx, sentence) in enumerate(sample_sentences.items(), 1):\n",
    "    sentences = sentence.split(\". \")\n",
    "    display_text = \". \".join(sentences[:3]) + (\".\" if len(sentences) >= 3 else \"\")\n",
    "    if len(sentences) > 3:\n",
    "        display_text += \" ...\"\n",
    "    print(f\"Sample {i}: \\\"{display_text}\\\"\")\n",
    "    print()\n",
    "\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# ------------------- Interactive Inference Section -------------------\n",
    "print(\"\\nInteractive Inference\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Instructions:\")\n",
    "print(\"  The samples listed above are randomly selected from 'test_revised.json'.\")\n",
    "print(\"  Please enter a partial or full sentence from any of the 15 samples displayed.\")\n",
    "print(\"  Example: 'The Sims class destroyers were built'\")\n",
    "print(\"  To exit, type 'exit'.\")\n",
    "print(\"  Output will show true triplets and matching predicted triplets.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Enter sentence part (or 'exit'): \").strip()\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    matched_index = None\n",
    "    for idx, sentence in sample_sentences.items():\n",
    "        if user_input.lower() in sentence.lower():\n",
    "            matched_index = idx\n",
    "            break\n",
    "\n",
    "    if matched_index is not None:\n",
    "        sample_data = test_data[matched_index]\n",
    "        sample_embedding = test_embeddings[matched_index]\n",
    "        result = infer_sample(gnn_model, sample_data, sample_embedding, optimal_thresholds, \n",
    "                             label_binarizer, relation_mapping, user_input)\n",
    "\n",
    "        print(\"\\nInput Sentence:\")\n",
    "        print(f\"  \\\"{result['sentence']}\\\"\")\n",
    "        print(\"True Triplets:\")\n",
    "        for head, relation, tail in result['true_triplets']:\n",
    "            print(f\"  ({head}, {relation}, {tail})\")\n",
    "        print(\"Predicted Triplets (Matching True Triplets):\")\n",
    "        if result['predicted_triplets']:\n",
    "            for head, relation, tail in result['predicted_triplets']:\n",
    "                print(f\"  ({head}, {relation}, {tail})\")\n",
    "        else:\n",
    "            print(\"  No matching triplets predicted.\")\n",
    "        print(f\"Match Count: {result['match_count'][0]}/{result['match_count'][1]} true triplets correctly predicted \")\n",
    "    else:\n",
    "        print(\"\\nNo match found. Please enter a part of a sentence from the list above.\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac587e58-bc1d-4132-b375-197e95b39203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
