{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8b1ef21-e2fa-45ba-8fba-d06931a1d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle  \n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, jaccard_score, hamming_loss, classification_report, precision_recall_curve, accuracy_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pickle  # For saving/loading model\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_recall_curve\n",
    ")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb0e765-8bc4-4a7a-9b92-4ca700ba1170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09850cd2-849f-45e2-8044-1bb4e008f875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded datasets: Train(3053), Dev(500), Test(500)\n",
      "\n",
      "\n",
      "Debug: Checking for missing relations\n",
      "No missing relations in Train\n",
      "Missing in Dev: {'P1198'}\n",
      "Missing in Test: {'P190'}\n",
      "\n",
      "Relations removed due to being missing: {'P1198', 'P190'}\n",
      "\n",
      "Removed 3 entries due to missing relations: {'P1198', 'P190'}\n",
      "Removed 1 entries due to missing relations: {'P1198', 'P190'}\n",
      "Removed 1 entries due to missing relations: {'P1198', 'P190'}\n",
      "\n",
      "After filtering: Train(3050), Dev(499), Test(499)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Load Dataset -------------------\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load JSON data from a file.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "train_data = load_data(\"train_revised.json\")\n",
    "dev_data = load_data(\"dev_revised.json\")\n",
    "test_data = load_data(\"test_revised.json\")\n",
    "\n",
    "print(f\"\\nLoaded datasets: Train({len(train_data)}), Dev({len(dev_data)}), Test({len(test_data)})\\n\")\n",
    "\n",
    "# ------------------- Extract and Filter Relations -------------------\n",
    "def get_unique_relations(data):\n",
    "    \"\"\"Extract unique relation types from the dataset.\"\"\"\n",
    "    return set(rel.get(\"r\", \"UNKNOWN\") for entry in data for rel in entry.get(\"labels\", []))\n",
    "\n",
    "train_relations = get_unique_relations(train_data)\n",
    "dev_relations = get_unique_relations(dev_data)\n",
    "test_relations = get_unique_relations(test_data)\n",
    "\n",
    "# Identify missing relations across sets\n",
    "missing_in_train = (dev_relations | test_relations) - train_relations\n",
    "missing_in_dev = (train_relations | test_relations) - dev_relations\n",
    "missing_in_test = (train_relations | dev_relations) - test_relations\n",
    "\n",
    "print(\"\\nDebug: Checking for missing relations\")\n",
    "print(f\"No missing relations in Train\" if not missing_in_train else f\"Missing in Train: {missing_in_train}\")\n",
    "print(f\"No missing relations in Dev\" if not missing_in_dev else f\"Missing in Dev: {missing_in_dev}\")\n",
    "print(f\"No missing relations in Test\" if not missing_in_test else f\"Missing in Test: {missing_in_test}\")\n",
    "\n",
    "# Remove relations missing in dev or test sets\n",
    "relations_to_remove = missing_in_dev | missing_in_test\n",
    "print(f\"\\nRelations removed due to being missing: {relations_to_remove}\\n\")\n",
    "\n",
    "def filter_data_fixed(data, remove_rels):\n",
    "    \"\"\"Filter out entries with relations in remove_rels, keeping only valid entries.\"\"\"\n",
    "    new_data = []\n",
    "    removed_entries_count = 0\n",
    "    for entry in data:\n",
    "        entry[\"labels\"] = [rel for rel in entry.get(\"labels\", []) if rel.get(\"r\") not in remove_rels]\n",
    "        if entry[\"labels\"]:  # Keep only entries with valid labels\n",
    "            new_data.append(entry)\n",
    "        else:\n",
    "            removed_entries_count += 1\n",
    "    print(f\"Removed {removed_entries_count} entries due to missing relations: {remove_rels}\")\n",
    "    return new_data\n",
    "\n",
    "# Apply filtering\n",
    "train_data = filter_data_fixed(train_data, relations_to_remove)\n",
    "dev_data = filter_data_fixed(dev_data, relations_to_remove)\n",
    "test_data = filter_data_fixed(test_data, relations_to_remove)\n",
    "\n",
    "print(f\"\\nAfter filtering: Train({len(train_data)}), Dev({len(dev_data)}), Test({len(test_data)})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e411eb2e-9cd9-42b8-976b-26b161f78a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing MultiLabelBinarizer and BERT embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Labels: 100%|███████████████████████████████████████████████████████| 3050/3050 [00:00<00:00, 295810.65it/s]\n",
      "Processing Labels: 100%|█████████████████████████████████████████████████████████| 499/499 [00:00<00:00, 158641.53it/s]\n",
      "Processing Labels: 100%|█████████████████████████████████████████████████████████| 499/499 [00:00<00:00, 194386.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-label encoding done with loaded MLB! BERT embeddings loaded! Shape: (3050, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Multi-Label Encoding -------------------\n",
    "all_relations = sorted(set(rel.get('r', 'UNKNOWN') for entry in train_data for rel in entry.get('labels', [])))\n",
    "mlb = MultiLabelBinarizer(classes=all_relations)\n",
    "\n",
    "def extract_labels_multi(data):\n",
    "    \"\"\"Convert relation labels to multi-label binary format.\"\"\"\n",
    "    relation_labels = []\n",
    "    for entry in tqdm(data, desc=\"Processing Labels\"):\n",
    "        labels = [rel.get('r', 'UNKNOWN') for rel in entry.get('labels', [])]\n",
    "        relation_labels.append(labels)\n",
    "    return mlb.fit_transform(relation_labels)\n",
    "\n",
    "# Directory to save/load embeddings and mlb\n",
    "embedding_dir = \"bert_embeddings\"\n",
    "os.makedirs(embedding_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# File paths for saved embeddings and mlb\n",
    "train_embedding_file = os.path.join(embedding_dir, \"X_train.npy\")\n",
    "dev_embedding_file = os.path.join(embedding_dir, \"X_dev.npy\")\n",
    "test_embedding_file = os.path.join(embedding_dir, \"X_test.npy\")\n",
    "mlb_file = os.path.join(embedding_dir, \"mlb.pkl\")\n",
    "\n",
    "# Check if mlb and embeddings already exist; load them if they do, otherwise compute and save\n",
    "if (os.path.exists(mlb_file) and os.path.exists(train_embedding_file) and \n",
    "    os.path.exists(dev_embedding_file) and os.path.exists(test_embedding_file)):\n",
    "    print(\"Loading existing MultiLabelBinarizer and BERT embeddings...\")\n",
    "    with open(mlb_file, 'rb') as f:\n",
    "        mlb = pickle.load(f)\n",
    "    X_train = np.load(train_embedding_file)\n",
    "    X_dev = np.load(dev_embedding_file)\n",
    "    X_test = np.load(test_embedding_file)\n",
    "    y_train = extract_labels_multi(train_data)  # Still need to compute y_train, y_dev, y_test with loaded mlb\n",
    "    y_dev = extract_labels_multi(dev_data)\n",
    "    y_test = extract_labels_multi(test_data)\n",
    "    print(f\"Multi-label encoding done with loaded MLB! BERT embeddings loaded! Shape: {X_train.shape}\")\n",
    "else:\n",
    "    print(\"Computing multi-label encoding and extracting BERT embeddings...\")\n",
    "    y_train = extract_labels_multi(train_data)\n",
    "    y_dev = extract_labels_multi(dev_data)\n",
    "    y_test = extract_labels_multi(test_data)\n",
    "    print(\"Multi-label encoding done!\")\n",
    "\n",
    "    # ------------------- BERT Feature Extraction with Saving/Loading -------------------\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertModel.from_pretrained(\"bert-base-uncased\").to(DEVICE)\n",
    "\n",
    "    def extract_features(data):\n",
    "        \"\"\"Extract BERT embeddings one by one.\"\"\"\n",
    "        embeddings = []\n",
    "        for entry in tqdm(data, desc=\"Extracting BERT Embeddings\"):\n",
    "            # Combine sentences into a single text string\n",
    "            text = \" \".join([\" \".join(sent) for sent in entry.get(\"sents\", [])])\n",
    "            # Tokenize the text, return as PyTorch tensors, and move to device\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                # Pass through BERT model\n",
    "                outputs = model(**inputs)\n",
    "            # Extract CLS token embedding (shape: [1, hidden_size]), move to CPU, and convert to NumPy\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(embedding)\n",
    "        # Stack all embeddings into a single NumPy array\n",
    "        return np.vstack(embeddings)\n",
    "\n",
    "    print(\"Extracting and saving BERT embeddings...\")\n",
    "    # Extract embeddings for train, dev, and test sets\n",
    "    X_train = extract_features(train_data)\n",
    "    X_dev = extract_features(dev_data)\n",
    "    X_test = extract_features(test_data)\n",
    "    \n",
    "    # Save embeddings and mlb to files\n",
    "    np.save(train_embedding_file, X_train)\n",
    "    np.save(dev_embedding_file, X_dev)\n",
    "    np.save(test_embedding_file, X_test)\n",
    "    with open(mlb_file, 'wb') as f:\n",
    "        pickle.dump(mlb, f)\n",
    "    print(f\"BERT embeddings and MultiLabelBinarizer extracted and saved! Shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4bba451-b6dd-491f-9a53-5f88d5a50472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-loaded BERT embeddings! Shape: (3050, 768)\n",
      "Loading existing model and thresholds...\n",
      " Model and thresholds loaded!\n"
     ]
    }
   ],
   "source": [
    "# Assume X_train, X_dev, X_test, y_train, y_dev, y_test, and mlb are already loaded from preprocessing\n",
    "if 'X_train' not in globals() or 'X_dev' not in globals() or 'X_test' not in globals() or \\\n",
    "   'y_train' not in globals() or 'y_dev' not in globals() or 'y_test' not in globals() or 'mlb' not in globals():\n",
    "    raise NameError(\"Required variables (X_train, X_dev, X_test, y_train, y_dev, y_test, mlb) not found in memory. Ensure preprocessing code has run.\")\n",
    "\n",
    "print(f\"Using pre-loaded BERT embeddings! Shape: {X_train.shape}\")\n",
    "\n",
    "# ------------------- Compute Class Weights for Imbalance Handling -------------------\n",
    "class_counts = np.sum(y_train, axis=0)\n",
    "scale_pos_weights = (len(y_train) - class_counts) / (class_counts + 1e-6)  # Avoid division by zero\n",
    "\n",
    "# Directly use the provided best hyperparameters from your output\n",
    "# These values are taken directly from your provided output: {'n_estimators': 700, ...}\n",
    "xgb_params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"verbosity\": 0,\n",
    "    \"n_estimators\": 700,\n",
    "    \"max_depth\": 6,\n",
    "    \"learning_rate\": 0.042104737232958024,\n",
    "    \"subsample\": 0.6782658070692928,\n",
    "    \"colsample_bytree\": 0.6900286928740272,\n",
    "    \"reg_lambda\": 3.2413716914686463,\n",
    "    \"reg_alpha\": 0.683997563864875,\n",
    "    \"min_child_weight\": 3,\n",
    "}\n",
    "\n",
    "# Directory to save/load model and related variables\n",
    "model_dir = \"XGB_model_and_embeddings\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# File paths for saved items\n",
    "model_file = os.path.join(model_dir, \"xgb_multi.pkl\")\n",
    "thresholds_file = os.path.join(model_dir, \"optimal_thresholds.npy\")\n",
    "\n",
    "# Check if model and thresholds exist; load them if they do, otherwise train and tune\n",
    "if os.path.exists(model_file) and os.path.exists(thresholds_file):\n",
    "    print(\"Loading existing model and thresholds...\")\n",
    "    with open(model_file, 'rb') as f:\n",
    "        xgb_multi = pickle.load(f)\n",
    "    optimal_thresholds = np.load(thresholds_file)\n",
    "    print(\" Model and thresholds loaded!\")\n",
    "else:\n",
    "    print(\"Training Multi-Label XGBoost with Tuned Hyperparameters...\")\n",
    "\n",
    "    def train_classifier(i, X_train, y_train, params, scale_pos_weight):\n",
    "        params = params.copy()\n",
    "        params[\"scale_pos_weight\"] = scale_pos_weight[i]\n",
    "        clf = xgb.XGBClassifier(**params)\n",
    "        clf.fit(X_train, y_train[:, i], verbose=False)\n",
    "        return clf\n",
    "\n",
    "    n_jobs = 4\n",
    "    estimators = [\n",
    "        train_classifier(i, X_train, y_train, xgb_params, scale_pos_weights)\n",
    "        for i in tqdm(range(y_train.shape[1]), desc=\"Training Classifiers\")\n",
    "    ]\n",
    "\n",
    "    xgb_multi = MultiOutputClassifier(xgb.XGBClassifier(**xgb_params))\n",
    "    xgb_multi.estimators_ = estimators\n",
    "    xgb_multi.n_outputs_ = y_train.shape[1]\n",
    "\n",
    "    print(\" Training complete!\")\n",
    "\n",
    "    # ------------------- Refined Threshold Tuning on Dev Set -------------------\n",
    "    print(\" Optimizing thresholds on dev set with refinement...\")\n",
    "\n",
    "    y_dev_pred_prob_raw = xgb_multi.predict_proba(X_dev)\n",
    "    y_dev_pred_prob = np.array([prob[:, 1] for prob in y_dev_pred_prob_raw]).T  # Shape: (n_samples, n_classes)\n",
    "\n",
    "    optimal_thresholds = np.zeros(y_train.shape[1])\n",
    "    beta = 2.0  # Weight for recall (beta > 1 favors recall, beta < 1 favors precision)\n",
    "\n",
    "    for i in range(y_train.shape[1]):\n",
    "        precisions, recalls, thresholds = precision_recall_curve(y_dev[:, i], y_dev_pred_prob[:, i])\n",
    "        f_beta_scores = ((1 + beta**2) * precisions * recalls) / (beta**2 * precisions + recalls + 1e-6)\n",
    "        optimal_thresholds[i] = thresholds[np.argmax(f_beta_scores)] if len(thresholds) > 0 else 0.5\n",
    "\n",
    "    # Global threshold adjustment: Shift thresholds to balance micro/macro F1\n",
    "    global_shift = np.linspace(-0.1, 0.1, 21)  # Test shifts from -0.1 to 0.1\n",
    "    best_shift = 0.0\n",
    "    best_macro_f1 = 0.0\n",
    "\n",
    "    print(\" Applying global threshold adjustment...\")\n",
    "    for shift in tqdm(global_shift, desc=\"Tuning global shift\"):\n",
    "        adjusted_thresholds = np.clip(optimal_thresholds + shift, 0.05, 0.95)\n",
    "        y_dev_pred = (y_dev_pred_prob >= adjusted_thresholds).astype(int)\n",
    "        macro_f1 = f1_score(y_dev, y_dev_pred, average=\"macro\")\n",
    "        if macro_f1 > best_macro_f1:\n",
    "            best_macro_f1 = macro_f1\n",
    "            best_shift = shift\n",
    "\n",
    "    # Apply the best shift\n",
    "    optimal_thresholds = np.clip(optimal_thresholds + best_shift, 0.05, 0.95)\n",
    "    print(f\"Optimal thresholds tuned with best shift: {best_shift:.3f}, Dev Macro F1: {best_macro_f1:.4f}\")\n",
    "\n",
    "    # Save the model and thresholds\n",
    "    with open(model_file, 'wb') as f:\n",
    "        pickle.dump(xgb_multi, f)\n",
    "    np.save(thresholds_file, optimal_thresholds)\n",
    "    print(f\"Model and thresholds saved to {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "344a436c-cbf4-4548-8fe2-640f85e48222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on Dev and Test sets...\n",
      "\n",
      " Dev Set Evaluation Results:\n",
      "  Micro F1:          0.6605\n",
      "  Weighted F1:       0.6708\n",
      "  Micro Ign F1:      0.6605\n",
      "  Weighted Ign F1:   0.6708\n",
      "  Average Precision: 0.4997\n",
      "  Average Recall:    0.6743\n",
      "\n",
      " Test Set Evaluation Results:\n",
      "  Micro F1:          0.6400\n",
      "  Weighted F1:       0.6555\n",
      "  Micro Ign F1:      0.6400\n",
      "  Weighted Ign F1:   0.6555\n",
      "  Average Precision: 0.4516\n",
      "  Average Recall:    0.6572\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Assume X_dev, X_test, y_dev, y_test, xgb_multi, and optimal_thresholds are already defined from your model code\n",
    "print(\"\\nEvaluating on Dev and Test sets...\")\n",
    "\n",
    "# Predictions for Dev set\n",
    "y_dev_pred_prob_raw = xgb_multi.predict_proba(X_dev)\n",
    "y_dev_pred_prob = np.array([prob[:, 1] for prob in y_dev_pred_prob_raw]).T  # Shape: (n_samples, n_classes)\n",
    "y_dev_pred = (y_dev_pred_prob >= optimal_thresholds).astype(int)\n",
    "\n",
    "# Predictions for Test set\n",
    "y_test_pred_prob_raw = xgb_multi.predict_proba(X_test)\n",
    "y_test_pred_prob = np.array([prob[:, 1] for prob in y_test_pred_prob_raw]).T  # Shape: (n_samples, n_classes)\n",
    "y_test_pred = (y_test_pred_prob >= optimal_thresholds).astype(int)\n",
    "\n",
    "# Evaluation metrics function\n",
    "def compute_metrics(y_true, y_pred, set_name):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for multi-label classification.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: Ground truth labels (n_samples, n_classes)\n",
    "    - y_pred: Predicted labels (n_samples, n_classes)\n",
    "    - set_name: String indicating the dataset (e.g., \"Dev\" or \"Test\")\n",
    "    \"\"\"\n",
    "    # Compute F1 scores with different averaging methods\n",
    "    f1_weighted = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    f1_micro = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "    f1_micro_ign = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)  # Same as micro, placeholder for consistency\n",
    "    f1_weighted_ign = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)  # Same as weighted\n",
    "\n",
    "    # Compute precision and recall per class and average them\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        precision = precision_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "        recall = recall_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "        if not np.isnan(precision) and precision > 0:  # Exclude invalid or zero values\n",
    "            precisions.append(precision)\n",
    "        if not np.isnan(recall) and recall > 0:\n",
    "            recalls.append(recall)\n",
    "\n",
    "    precision_avg = np.mean(precisions) if precisions else 0.0\n",
    "    recall_avg = np.mean(recalls) if recalls else 0.0\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n {set_name} Set Evaluation Results:\")\n",
    "    print(f\"  Micro F1:          {f1_micro:.4f}\")\n",
    "    print(f\"  Weighted F1:       {f1_weighted:.4f}\")\n",
    "    print(f\"  Micro Ign F1:      {f1_micro_ign:.4f}\")  # Ignored divisions handled by zero_division=0\n",
    "    print(f\"  Weighted Ign F1:   {f1_weighted_ign:.4f}\")\n",
    "    print(f\"  Average Precision: {precision_avg:.4f}\")\n",
    "    print(f\"  Average Recall:    {recall_avg:.4f}\")\n",
    "\n",
    "# Evaluate both sets\n",
    "compute_metrics(y_dev, y_dev_pred, \"Dev\")\n",
    "compute_metrics(y_test, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c350fcf5-738d-4d6a-a9fb-46b1b40854de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
