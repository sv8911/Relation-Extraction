{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06072a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/latest_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Import Necessary Libraries -------------------\n",
    "# Import libraries for data handling, machine learning, and neural network operations\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, precision_score, recall_score\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.nn import GCNConv, knn_graph\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_undirected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb041fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Device Configuration -------------------\n",
    "# Set up the computation device: use MPS (Metal Performance Shaders) if available, otherwise default to CPU\n",
    "\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a96c507-4e75-4243-8d8e-3129649e0053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded datasets: Train(3053), Dev(500), Test(500)\n",
      "\n",
      "\n",
      "Debug: Checking for missing relations\n",
      "No missing relations in Train\n",
      "Missing in Dev: {'P1198'}\n",
      "Missing in Test: {'P190'}\n",
      "\n",
      "Relations removed due to being missing: {'P1198', 'P190'}\n",
      "\n",
      "Removed 3 entries due to missing relations: {'P1198', 'P190'}\n",
      "Removed 1 entries due to missing relations: {'P1198', 'P190'}\n",
      "Removed 1 entries due to missing relations: {'P1198', 'P190'}\n",
      "\n",
      "After filtering: Train(3050), Dev(499), Test(499)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Load Dataset -------------------\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load JSON data from a specified file path.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file to be loaded.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries containing the loaded data.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load the training, development, and test datasets from their respective JSON files\n",
    "train_data = load_data(\"train_revised.json\")\n",
    "dev_data = load_data(\"dev_revised.json\")\n",
    "test_data = load_data(\"test_revised.json\")\n",
    "\n",
    "# Display the size of each dataset for verification\n",
    "print(f\"\\nLoaded datasets: Train({len(train_data)}), Dev({len(dev_data)}), Test({len(test_data)})\\n\")\n",
    "\n",
    "# ------------------- Extract and Filter Relations -------------------\n",
    "def get_unique_relations(data):\n",
    "    \"\"\"Extract all unique relation types present in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        data (list): List of data entries, each containing a 'labels' field with relation dictionaries.\n",
    "    \n",
    "    Returns:\n",
    "        set: A set of unique relation identifiers (e.g., 'P123', 'NA').\n",
    "    \"\"\"\n",
    "    return set(rel.get(\"r\", \"UNKNOWN\") for entry in data for rel in entry.get(\"labels\", []))\n",
    "\n",
    "# Extract unique relations from each dataset\n",
    "train_relations = get_unique_relations(train_data)\n",
    "dev_relations = get_unique_relations(dev_data)\n",
    "test_relations = get_unique_relations(test_data)\n",
    "\n",
    "# Identify relations that are present in one dataset but missing in others\n",
    "missing_in_train = (dev_relations | test_relations) - train_relations\n",
    "missing_in_dev = (train_relations | test_relations) - dev_relations\n",
    "missing_in_test = (train_relations | dev_relations) - test_relations\n",
    "\n",
    "# Print debugging information about missing relations\n",
    "print(\"\\nDebug: Checking for missing relations\")\n",
    "print(f\"No missing relations in Train\" if not missing_in_train else f\"Missing in Train: {missing_in_train}\")\n",
    "print(f\"No missing relations in Dev\" if not missing_in_dev else f\"Missing in Dev: {missing_in_dev}\")\n",
    "print(f\"No missing relations in Test\" if not missing_in_test else f\"Missing in Test: {missing_in_test}\")\n",
    "\n",
    "# Combine relations to remove (those missing in any dataset) to ensure consistency across splits\n",
    "relations_to_remove = missing_in_dev | missing_in_test\n",
    "print(f\"\\nRelations removed due to being missing: {relations_to_remove}\\n\")\n",
    "\n",
    "def filter_data_fixed(data, remove_rels):\n",
    "    \"\"\"Filter out data entries containing specified relations to ensure dataset consistency.\n",
    "    \n",
    "    Args:\n",
    "        data (list): List of data entries to filter.\n",
    "        remove_rels (set): Set of relation identifiers to remove.\n",
    "    \n",
    "    Returns:\n",
    "        list: Filtered list of data entries with unwanted relations removed.\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "    removed_entries_count = 0\n",
    "    for entry in data:\n",
    "        # Keep only labels whose relation is not in the remove list\n",
    "        entry[\"labels\"] = [rel for rel in entry.get(\"labels\", []) if rel.get(\"r\") not in remove_rels]\n",
    "        if entry[\"labels\"]: # If there are still labels, keep the entry\n",
    "            new_data.append(entry)\n",
    "        else: # Otherwise, count it as removed\n",
    "            removed_entries_count += 1\n",
    "    print(f\"Removed {removed_entries_count} entries due to missing relations: {remove_rels}\")\n",
    "    return new_data\n",
    "\n",
    "# Apply the filtering function to all datasets\n",
    "train_data = filter_data_fixed(train_data, relations_to_remove)\n",
    "dev_data = filter_data_fixed(dev_data, relations_to_remove)\n",
    "test_data = filter_data_fixed(test_data, relations_to_remove)\n",
    "\n",
    "# Display the updated dataset sizes after filtering\n",
    "print(f\"\\nAfter filtering: Train({len(train_data)}), Dev({len(dev_data)}), Test({len(test_data)})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39a74a49-5038-4792-9a81-f9fce828c6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing MultiLabelBinarizer and BERT embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Labels: 100%|██████████| 3050/3050 [00:00<00:00, 157686.43it/s]\n",
      "Processing Labels: 100%|██████████| 499/499 [00:00<00:00, 159281.41it/s]\n",
      "Processing Labels: 100%|██████████| 499/499 [00:00<00:00, 141082.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-label encoding done with loaded MLB! BERT embeddings loaded! Shape: (3050, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Multi-Label Encoding -------------------\n",
    "# Create a sorted list of all unique relations in the training data for consistent encoding\n",
    "all_relations = sorted(set(rel.get('r', 'UNKNOWN') for entry in train_data for rel in entry.get('labels', [])))\n",
    "mlb = MultiLabelBinarizer(classes=all_relations)\n",
    "\n",
    "def extract_labels_multi(data):\n",
    "    \"\"\"Convert relation labels into a multi-label binary format using MultiLabelBinarizer.\n",
    "    \n",
    "    Args:\n",
    "        data (list): List of data entries with 'labels' field.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Binary matrix where each row represents an entry and each column a relation.\n",
    "    \"\"\"\n",
    "    relation_labels = []\n",
    "    for entry in tqdm(data, desc=\"Processing Labels\"):\n",
    "        labels = [rel.get('r', 'UNKNOWN') for rel in entry.get('labels', [])]\n",
    "        relation_labels.append(labels)\n",
    "    return mlb.fit_transform(relation_labels)\n",
    "\n",
    "# Set up directory to save or load embeddings and MultiLabelBinarizer object\n",
    "embedding_dir = \"bert_embeddings\"\n",
    "os.makedirs(embedding_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# Define file paths for saving/loading embeddings and the MultiLabelBinarizer\n",
    "train_embedding_file = os.path.join(embedding_dir, \"X_train.npy\")\n",
    "dev_embedding_file = os.path.join(embedding_dir, \"X_dev.npy\")\n",
    "test_embedding_file = os.path.join(embedding_dir, \"X_test.npy\")\n",
    "mlb_file = os.path.join(embedding_dir, \"mlb.pkl\")\n",
    "\n",
    "# Check if precomputed embeddings and MultiLabelBinarizer exist\n",
    "embeddings_exist = (os.path.exists(mlb_file) and os.path.exists(train_embedding_file) and \n",
    "                    os.path.exists(dev_embedding_file) and os.path.exists(test_embedding_file))\n",
    "\n",
    "if embeddings_exist:\n",
    "    # Load existing MultiLabelBinarizer and embeddings if all files are present\n",
    "    print(\"Loading existing MultiLabelBinarizer and BERT embeddings...\")\n",
    "    with open(mlb_file, 'rb') as f:\n",
    "        mlb = pickle.load(f)\n",
    "    X_train = np.load(train_embedding_file)\n",
    "    X_dev = np.load(dev_embedding_file)\n",
    "    X_test = np.load(test_embedding_file)\n",
    "    y_train = extract_labels_multi(train_data)  # Compute labels with loaded MultiLabelBinarizer\n",
    "    y_dev = extract_labels_multi(dev_data)\n",
    "    y_test = extract_labels_multi(test_data)\n",
    "    print(f\"Multi-label encoding done with loaded MLB! BERT embeddings loaded! Shape: {X_train.shape}\")\n",
    "else:\n",
    "    # Compute multi-label encoding and extract BERT embeddings if files are missing\n",
    "    print(\"Computing multi-label encoding and extracting BERT embeddings...\")\n",
    "    y_train = extract_labels_multi(train_data)\n",
    "    y_dev = extract_labels_multi(dev_data)\n",
    "    y_test = extract_labels_multi(test_data)\n",
    "    print(\"Multi-label encoding done!\")\n",
    "\n",
    "    # ------------------- BERT Feature Extraction -------------------\n",
    "    # Initialize the BERT tokenizer and model for feature extraction\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertModel.from_pretrained(\"bert-base-uncased\").to(DEVICE)\n",
    "\n",
    "    def extract_features(data):\n",
    "        \"\"\"Extract BERT embeddings for each data entry using the BERT model.\n",
    "        \n",
    "        Args:\n",
    "            data (list): List of data entries with 'sents' field containing sentences.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Stacked array of BERT embeddings (CLS token) for each entry.\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        for entry in tqdm(data, desc=\"Extracting BERT Embeddings\"):\n",
    "            # Combine all sentences in the entry into a single string\n",
    "            text = \" \".join([\" \".join(sent) for sent in entry.get(\"sents\", [])])\n",
    "            # Tokenize the text and prepare it for BERT input\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            # Extract the CLS token embedding from the last hidden state\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(embedding)\n",
    "        return np.vstack(embeddings)\n",
    "\n",
    "    # Extract BERT embeddings for all datasets since they weren't loaded\n",
    "    print(\"Extracting and saving BERT embeddings...\")\n",
    "    X_train = extract_features(train_data)\n",
    "    X_dev = extract_features(dev_data)\n",
    "    X_test = extract_features(test_data)\n",
    "\n",
    "    # Save the computed embeddings and MultiLabelBinarizer to files for future use\n",
    "    np.save(train_embedding_file, X_train)\n",
    "    np.save(dev_embedding_file, X_dev)\n",
    "    np.save(test_embedding_file, X_test)\n",
    "    with open(mlb_file, 'wb') as f:\n",
    "        pickle.dump(mlb, f)\n",
    "    print(f\"BERT embeddings extracted! Shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e38a1baf-32aa-4102-98d3-d9afc80f8695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model from GCN_model_optmizers/GCN_model.pth\n",
      "Best model loaded successfully!\n",
      "Optimizing thresholds on dev set...\n",
      "Optimizer saved to GCN_model_optmizers/optimizer.pth\n",
      "Evaluating on test set...\n",
      "\n",
      "Dev Metrics:\n",
      "Micro F1: 0.6012\n",
      "Weighted F1: 0.6495\n",
      "Micro IGN F1: 0.6012\n",
      "Weighted IGN F1: 0.6495\n",
      "Precision: 0.5156\n",
      "Recall: 0.7209\n",
      "\n",
      "Test Metrics:\n",
      "Micro F1: 0.5546\n",
      "Weighted F1: 0.6046\n",
      "Micro IGN F1: 0.5546\n",
      "Weighted IGN F1: 0.6046\n",
      "Precision: 0.4687\n",
      "Recall: 0.6789\n"
     ]
    }
   ],
   "source": [
    "# Set up directory to save the GCN model and optimizer\n",
    "model_dir  = \"GCN_model_optmizers\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "model_save_path = os.path.join(model_dir, \"GCN_model.pth\")\n",
    "optimizer_save_path = os.path.join(model_dir, \"optimizer.pth\")\n",
    "\n",
    "# Verify that required variables from preprocessing are available\n",
    "if 'X_train' not in globals() or 'X_dev' not in globals() or 'X_test' not in globals() or \\\n",
    "   'y_train' not in globals() or 'y_dev' not in globals() or 'y_test' not in globals() or 'mlb' not in globals():\n",
    "    raise NameError(\"Required variables (X_train, X_dev, X_test, y_train, y_dev, y_test, mlb) not found in memory. Ensure preprocessing code has run.\")\n",
    "\n",
    "# ------------------- Custom Multi-Label Focal Loss -------------------\n",
    "class MultiLabelFocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, weight=None, reduction='mean'):\n",
    "        \"\"\"Initialize the focal loss function for multi-label classification.\n",
    "        \n",
    "        Args:\n",
    "            gamma (float): Focusing parameter to reduce the impact of easy examples.\n",
    "            weight (torch.Tensor, optional): Weights for each class to handle imbalance.\n",
    "            reduction (str): Method to reduce the loss ('mean', 'sum', or None).\n",
    "        \"\"\"\n",
    "        super(MultiLabelFocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"Compute the focal loss between predictions and targets.\n",
    "        \n",
    "        Args:\n",
    "            inputs (torch.Tensor): Model predictions (logits).\n",
    "            targets (torch.Tensor): True binary labels.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Computed focal loss.\n",
    "        \"\"\"\n",
    "        ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        p_t = torch.sigmoid(inputs) * targets + (1 - torch.sigmoid(inputs)) * (1 - targets)\n",
    "        loss = ce_loss * ((1 - p_t) ** self.gamma)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            weight = self.weight[None, :]\n",
    "            loss = loss * (targets * weight + (1 - targets))\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "# ------------------- Enhanced GCN Model with Dropout -------------------\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim, dropout=0.4):\n",
    "        \"\"\"Initialize the Graph Convolutional Network (GCN) model.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Dimension of input features (BERT embeddings).\n",
    "            hidden_dim1 (int): Dimension of the first hidden layer.\n",
    "            hidden_dim2 (int): Dimension of the second hidden layer.\n",
    "            output_dim (int): Number of output classes (relations).\n",
    "            dropout (float): Dropout rate to prevent overfitting.\n",
    "        \"\"\"\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim1)\n",
    "        self.conv2 = GCNConv(hidden_dim1, hidden_dim2)\n",
    "        self.conv3 = GCNConv(hidden_dim2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"Forward pass through the GCN model.\n",
    "        \n",
    "        Args:\n",
    "            data (Data): Graph data object containing features and edge indices.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output logits for each node.\n",
    "        \"\"\"\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# ------------------- Graph Construction with k-NN -------------------\n",
    "def create_sparse_graph_data(X, y, k=3):\n",
    "    \"\"\"Construct a graph data object using k-nearest neighbors (k-NN).\n",
    "    \n",
    "    Args:\n",
    "        X (numpy.ndarray): Feature matrix (BERT embeddings).\n",
    "        y (numpy.ndarray): Label matrix (binary multi-label).\n",
    "        k (int): Number of nearest neighbors to connect in the graph.\n",
    "    \n",
    "    Returns:\n",
    "        Data: PyTorch Geometric Data object with nodes, edges, and labels.\n",
    "    \"\"\"\n",
    "    x = torch.tensor(X, dtype=torch.float)\n",
    "    edge_index = knn_graph(x, k=k, loop=False)\n",
    "    edge_index = to_undirected(edge_index)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    return Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Create graph data objects for training, development, and test sets\n",
    "train_graph = create_sparse_graph_data(X_train, y_train, k=3).to(DEVICE)\n",
    "dev_graph = create_sparse_graph_data(X_dev, y_dev, k=3).to(DEVICE)\n",
    "test_graph = create_sparse_graph_data(X_test, y_test, k=3).to(DEVICE)\n",
    "\n",
    "# Define GCN model parameters\n",
    "input_dim = X_train.shape[1]  # 768 (BERT embedding size)\n",
    "hidden_dim1 = 512\n",
    "hidden_dim2 = 256\n",
    "output_dim = y_train.shape[1] # Number of unique relations\n",
    "dropout_rate = 0.4\n",
    "gnn_model = GCN(input_dim, hidden_dim1, hidden_dim2, output_dim, dropout=dropout_rate).to(DEVICE)\n",
    "\n",
    "# Compute class weights to handle label imbalance\n",
    "class_counts = np.sum(y_train, axis=0)\n",
    "pos_weights = torch.tensor((len(y_train) - class_counts) / (class_counts + 1e-6), dtype=torch.float).to(DEVICE)\n",
    "\n",
    "# Initialize loss function and optimizer\n",
    "criterion = MultiLabelFocalLoss(gamma=2.0, weight=pos_weights, reduction='mean')\n",
    "optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Set up training monitoring variables\n",
    "train_f1_scores = []\n",
    "dev_f1_scores = []\n",
    "best_dev_f1 = 0\n",
    "patience = 10\n",
    "counter = 0\n",
    "\n",
    "# ------------------- Training Loop with Overfitting Check -------------------\n",
    "if os.path.exists(model_save_path):\n",
    "    print(f\"Loading existing model from {model_save_path}\")\n",
    "    gnn_model.load_state_dict(torch.load(model_save_path))\n",
    "    gnn_model.to(DEVICE)\n",
    "else:\n",
    "    print(\"Training GNN Model...\")\n",
    "    for epoch in tqdm(range(55), desc=\"Training Epochs\"):\n",
    "        gnn_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = gnn_model(train_graph)\n",
    "        loss = criterion(out, train_graph.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate model performance every 5 epochs\n",
    "        if epoch % 5 == 0:\n",
    "            gnn_model.eval()\n",
    "            with torch.no_grad():\n",
    "                train_logits = gnn_model(train_graph)\n",
    "                train_pred_prob = torch.sigmoid(train_logits).cpu().numpy()\n",
    "                train_preds = (train_pred_prob >= 0.5).astype(int)\n",
    "                train_f1 = f1_score(y_train, train_preds, average=\"micro\")\n",
    "\n",
    "                dev_logits = gnn_model(dev_graph)\n",
    "                dev_pred_prob = torch.sigmoid(dev_logits).cpu().numpy()\n",
    "                dev_preds = (dev_pred_prob >= 0.5).astype(int)\n",
    "                dev_f1 = f1_score(y_dev, dev_preds, average=\"micro\")\n",
    "\n",
    "            train_f1_scores.append(train_f1)\n",
    "            dev_f1_scores.append(dev_f1)\n",
    "            print(f\"Epoch {epoch}: Train F1 = {train_f1:.4f}, Dev F1 = {dev_f1:.4f}\")\n",
    "\n",
    "            # Implement early stopping based on development set F1 score\n",
    "            if dev_f1 > best_dev_f1:\n",
    "                best_dev_f1 = dev_f1\n",
    "                torch.save(gnn_model.state_dict(), model_save_path)\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "        torch.cuda.empty_cache() # Clear GPU memory if applicable\n",
    "\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Load best model if it exists\n",
    "if os.path.exists(model_save_path):\n",
    "    gnn_model.load_state_dict(torch.load(model_save_path))\n",
    "    print(\"Best model loaded successfully!\")\n",
    "else:\n",
    "    print(\"Model file not found. Please ensure the model is trained and saved.\")\n",
    "\n",
    "# ------------------- Threshold Tuning -------------------\n",
    "print(\"Optimizing thresholds on dev set...\")\n",
    "gnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    dev_logits = gnn_model(dev_graph)\n",
    "    dev_pred_prob = torch.sigmoid(dev_logits).cpu().numpy()\n",
    "\n",
    "# Tune thresholds for each class to maximize F1 score on the development set\n",
    "optimal_thresholds = np.zeros(output_dim)\n",
    "for i in range(output_dim):\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_dev[:, i], dev_pred_prob[:, i])\n",
    "    f1_scores = (2 * precisions * recalls) / (precisions + recalls + 1e-6)\n",
    "    optimal_thresholds[i] = thresholds[np.argmax(f1_scores)] if len(thresholds) > 0 else 0.5\n",
    "\n",
    "# Save optimal thresholds for later use in real-time inference\n",
    "torch.save(optimizer.state_dict(), optimizer_save_path)\n",
    "print(f\"Optimizer saved to {optimizer_save_path}\")\n",
    "\n",
    "# Define relations to ignore during evaluation\n",
    "IGNORED_RELATIONS = {\"NA\"}\n",
    "\n",
    "def compute_ign_f1(y_true, y_pred, relation_list):\n",
    "    \"\"\"Compute F1 scores while ignoring specified relations.\n",
    "    \n",
    "    Args:\n",
    "        y_true (numpy.ndarray): True binary labels.\n",
    "        y_pred (numpy.ndarray): Predicted binary labels.\n",
    "        relation_list (list): List of all relation types.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Micro and weighted IGN F1 scores.\n",
    "    \"\"\"\n",
    "    relevant_indices = [i for i, rel in enumerate(relation_list) if rel not in IGNORED_RELATIONS]\n",
    "    \n",
    "    y_true_filtered = y_true[:, relevant_indices]\n",
    "    y_pred_filtered = y_pred[:, relevant_indices]\n",
    "    \n",
    "    ign_f1_micro = f1_score(y_true_filtered, y_pred_filtered, average=\"micro\", zero_division=0)\n",
    "    ign_f1_weighted = f1_score(y_true_filtered, y_pred_filtered, average=\"weighted\", zero_division=0)\n",
    "    \n",
    "    return ign_f1_micro, ign_f1_weighted\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "with torch.no_grad():\n",
    "    dev_logits = gnn_model(dev_graph)\n",
    "    test_logits = gnn_model(test_graph)\n",
    "\n",
    "    dev_pred_prob = torch.sigmoid(dev_logits).cpu().numpy()\n",
    "    test_pred_prob = torch.sigmoid(test_logits).cpu().numpy()\n",
    "\n",
    "    # Apply optimized thresholds to get binary predictions\n",
    "    y_dev_pred = (dev_pred_prob >= optimal_thresholds).astype(int)\n",
    "    y_test_pred = (test_pred_prob >= optimal_thresholds).astype(int)\n",
    "\n",
    "# Define datasets and their true/predicted labels for evaluation\n",
    "metrics = {\n",
    "    \"Dev\": {\"y_true\": y_dev, \"y_pred\": y_dev_pred},\n",
    "    \"Test\": {\"y_true\": y_test, \"y_pred\": y_test_pred}\n",
    "}\n",
    "\n",
    "# Compute and display evaluation metrics for each dataset\n",
    "for dataset, data in metrics.items():\n",
    "    y_true, y_pred = data[\"y_true\"], data[\"y_pred\"]\n",
    "    \n",
    "    f1_micro = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "    f1_weighted = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    precision = precision_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "    \n",
    "    ign_f1_micro, ign_f1_weighted = compute_ign_f1(y_true, y_pred, all_relations)\n",
    "\n",
    "    print(f\"\\n{dataset} Metrics:\")\n",
    "    print(f\"Micro F1: {f1_micro:.4f}\")\n",
    "    print(f\"Weighted F1: {f1_weighted:.4f}\")\n",
    "    print(f\"Micro IGN F1: {ign_f1_micro:.4f}\")\n",
    "    print(f\"Weighted IGN F1: {ign_f1_weighted:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49acb14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latest_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
